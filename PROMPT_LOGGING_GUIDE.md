# Guide du Logging des Prompts DSPy avec MLflow

Ce guide explique comment utiliser le syst√®me de logging des prompts int√©gr√© dans votre projet Kouskous 2025.

## üéØ Fonctionnalit√©s du Logging des Prompts

### ‚úÖ Ce qui est automatiquement logg√©

1. **Prompts complets** envoy√©s au LLM
2. **R√©ponses compl√®tes** re√ßues du LLM  
3. **M√©triques de performance** : 
   - Longueur des prompts/r√©ponses
   - Estimation des tokens utilis√©s
   - Temps de traitement
   - Nombre d'appels total

4. **Contexte d'extraction** :
   - Contenu complet du contexte
   - Nombre de pages trait√©es
   - Mots-cl√©s d√©tect√©s

5. **Historique complet** sauvegard√© comme artefacts MLflow

## üìä M√©triques disponibles dans MLflow

### M√©triques par appel LLM
- `llm_call_{N}_prompt_length` : Longueur du prompt N
- `llm_call_{N}_response_length` : Longueur de la r√©ponse N  
- `llm_call_{N}_tokens` : Tokens estim√©s pour l'appel N

### M√©triques cumulatives
- `total_llm_calls` : Nombre total d'appels
- `total_estimated_tokens` : Total des tokens estim√©s
- `final_avg_prompt_length` : Longueur moyenne des prompts
- `final_unique_prompts` : Nombre de prompts uniques

### M√©triques du contexte
- `extraction_context_length` : Longueur du contexte
- `extraction_context_lines` : Nombre de lignes
- `context_keyword_{mot}` : Occurrences de mots-cl√©s

## üìÇ Artefacts sauvegard√©s

### Dans l'onglet "Artifacts" de MLflow :

1. **`prompts/`** : Dossier contenant tous les prompts
   - `prompts_log.json` : Historique complet des appels

2. **`dspy_calls/`** : Dossier des appels DSPy  
   - `dspy_calls.json` : D√©tails des signatures DSPy

## üîç Comment consulter les prompts

### 1. Interface MLflow Web
```bash
uv run python launch_mlflow.py
# Ouvrir http://localhost:5000
```

Dans l'interface :
1. S√©lectionner l'exp√©rience "kouskous2025_extraction"
2. Cliquer sur un run
3. Aller dans l'onglet "Artifacts" 
4. T√©l√©charger `prompts/prompts_log.json`

### 2. Dashboard programmatique
```bash
uv run python mlflow_dashboard.py
```

### 3. Acc√®s direct aux artefacts
```python
import mlflow
import json

# R√©cup√©rer le dernier run
runs = mlflow.search_runs(experiment_ids=[experiment_id], max_results=1)
run_id = runs.iloc[0]['run_id']

# T√©l√©charger l'artefact des prompts
mlflow.artifacts.download_artifacts(
    run_id=run_id, 
    artifact_path="prompts/prompts_log.json",
    dst_path="./downloaded_prompts/"
)

# Lire les prompts
with open("./downloaded_prompts/prompts_log.json", "r") as f:
    prompts_data = json.load(f)
    
for prompt_entry in prompts_data:
    print(f"Prompt {prompt_entry['call_number']}:")
    print(prompt_entry['prompt'])
    print(f"R√©ponse: {prompt_entry['response']}")
    print("-" * 50)
```

## üìã Structure des donn√©es de prompts

### Format JSON des prompts logg√©s :
```json
{
  "call_number": 1,
  "timestamp": "2025-08-24T23:31:43.123456",
  "prompt_hash": "abc12345",
  "prompt": "Texte complet du prompt envoy√©...",
  "response": "R√©ponse compl√®te re√ßue...",
  "model": "google/gemini-2.5-flash",
  "prompt_length": 1234,
  "response_length": 5678,
  "estimated_prompt_tokens": 300,
  "estimated_response_tokens": 1200,
  "total_estimated_tokens": 1500,
  "metadata": {
    "call_duration": 2.5,
    "model_name": "openrouter/google/gemini-2.5-flash",
    "args_count": 2,
    "kwargs_keys": ["temperature", "max_tokens"]
  }
}
```

## üîß Configuration avanc√©e

### Personnaliser le logging
```python
# Dans votre script
from prompt_logger import prompt_logger

# Logger manuellement un appel
prompt_logger.log_llm_call(
    prompt="Mon prompt personnalis√©",
    response="La r√©ponse obtenue", 
    model="mon-modele",
    metadata={"custom": "data"}
)

# Obtenir les statistiques
stats = prompt_logger.get_statistics()
print(f"Appels total: {stats['total_calls']}")
```

### D√©sactiver le logging
```python
# Pour d√©sactiver temporairement
# Commentez cette ligne dans main.py :
# setup_dspy_prompt_logging()
```

## üìà Analyser les co√ªts

### Estimation des co√ªts par mod√®le :
```python
def estimate_cost(tokens, model_name):
    # Prix approximatifs (√† ajuster selon votre fournisseur)
    prices = {
        "google/gemini-2.5-flash": 0.0001,  # par 1000 tokens
        "anthropic/claude-3": 0.0015,
        "openai/gpt-4": 0.003
    }
    
    price_per_1k = prices.get(model_name, 0.0001)
    return (tokens / 1000) * price_per_1k

# Dans vos analyses MLflow
total_tokens = runs['metrics.total_estimated_tokens'].sum()
estimated_cost = estimate_cost(total_tokens, "google/gemini-2.5-flash")
print(f"Co√ªt estim√©: ${estimated_cost:.4f}")
```

## üö® Bonnes pratiques

### ‚úÖ √Ä faire :
- Consultez r√©guli√®rement les prompts pour optimiser leur efficacit√©
- Surveillez la longueur des prompts pour contr√¥ler les co√ªts
- Analysez les patterns de r√©ponses pour am√©liorer les signatures DSPy
- Comparez les performances entre diff√©rents mod√®les

### ‚ùå √Ä √©viter :
- Ne pas inclure de donn√©es sensibles dans les prompts
- Ne pas laisser les prompts devenir trop longs inutilement
- Ne pas oublier de nettoyer les anciens runs MLflow p√©riodiquement

## üìä M√©triques recommand√©es √† surveiller

1. **Efficacit√©** : tokens/restaurant extrait
2. **Coh√©rence** : nombre de prompts uniques vs total
3. **Performance** : temps moyen par appel LLM
4. **Qualit√©** : taux de succ√®s d'extraction vs longueur prompts

## üîç D√©pannage

### Prompts non sauvegard√©s
```bash
# V√©rifier que MLflow est configur√©
uv run python test_prompt_logging.py

# V√©rifier les permissions du dossier mlruns
ls -la mlruns/
```

### M√©triques manquantes
```python
# V√©rifier que le logging est activ√©
from prompt_logger import prompt_logger
print(f"Appels logg√©s: {len(prompt_logger.prompts_log)}")
```

### Artefacts corrompus
```bash
# Supprimer et relancer
rm -rf mlruns/
uv run python main.py
```

## üí° Optimisations sugg√©r√©es

### R√©duire les co√ªts
1. Analyser les prompts redondants
2. Optimiser le contexte d'extraction
3. Utiliser le cache DSPy efficacement
4. Ajuster max_tokens selon les besoins

### Am√©liorer la qualit√©
1. Analyser les √©checs d'extraction
2. Comparer les prompts efficaces vs inefficaces  
3. Ajuster les signatures DSPy selon les patterns observ√©s
4. Tester diff√©rentes formulations de prompts

Ce syst√®me vous donne une visibilit√© compl√®te sur l'utilisation de votre LLM et vous permet d'optimiser continuellement vos extractions ! üöÄ
